import os
import json
import subprocess
import datetime
import shutil

# å®šä¹‰å·¥ä½œç›®å½•
WORKSPACE_DIR = "workspace"
OLD_STATS_FILE = "old_data/stats.json"
# æ–°çš„ stats æ–‡ä»¶å­˜æ”¾åœ¨ workspace æ ¹ç›®å½•ï¼Œç”¨äºŽæŽ¨é€åˆ°ä»“åº“
STATS_FILE = os.path.join(WORKSPACE_DIR, "stats.json")

def run_command(cmd):
    """è¿è¡Œç³»ç»Ÿå‘½ä»¤"""
    try:
        subprocess.check_call(cmd, shell=True, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        print(f"âš ï¸ Warning: Command failed: {cmd}")

def count_lines(filepath):
    """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
    try:
        with open(filepath, 'rb') as f:
            return sum(1 for _ in f)
    except:
        return 0

def process_dat_files():
    """éåŽ†ç›®å½•ï¼Œè§£åŒ… dat æ–‡ä»¶ï¼Œå¹¶è¿”å›žç»Ÿè®¡æ•°æ®"""
    current_stats = {}
    
    # éåŽ† workspace ä¸‹çš„æ‰€æœ‰ä½œè€…ç›®å½•
    for author in os.listdir(WORKSPACE_DIR):
        author_path = os.path.join(WORKSPACE_DIR, author)
        # æŽ’é™¤éžæ–‡ä»¶å¤¹æˆ–éšè—æ–‡ä»¶å¤¹
        if not os.path.isdir(author_path) or author.startswith("."):
            continue
            
        print(f"ðŸ” Analyzing {author}...")
        current_stats[author] = {}

        # éåŽ†ä½œè€…ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ (geoip, geosite)
        for category in ["geoip", "geosite"]:
            cat_dir = os.path.join(author_path, category)
            if not os.path.exists(cat_dir):
                continue
                
            # æ‰¾åˆ°ç›®å½•ä¸‹çš„ .dat æ–‡ä»¶
            for file in os.listdir(cat_dir):
                if not file.endswith(".dat"):
                    continue
                
                dat_path = os.path.join(cat_dir, file)
                # åˆ›å»ºå¯¼å‡ºç›®å½• (ä¾‹å¦‚ workspace/MetaCubeX/geoip/geoip_text)
                # æ³¨æ„ï¼šä¸ºäº†ç›®å½•æ•´æ´ï¼Œå»ºè®®æŠŠè§£åŒ…çš„æ–‡æœ¬æ”¾åœ¨å•ç‹¬æ–‡ä»¶å¤¹ï¼Œé¿å…æ±¡æŸ“
                export_dir = os.path.join(cat_dir, f"{file}_text")
                if os.path.exists(export_dir):
                    shutil.rmtree(export_dir)
                os.makedirs(export_dir, exist_ok=True)
                
                print(f"  -> Extracting {file}...")
                
                mode = "geoip" if "geoip" in file.lower() else "geosite"
                
                try:
                    # ä½¿ç”¨ v2dat è§£åŒ…
                    run_command(f"v2dat unpack {mode} -o {export_dir} {dat_path}")
                    
                    # ç»Ÿè®¡è§£åŒ…åŽçš„æ–‡ä»¶
                    if os.path.exists(export_dir):
                        files = os.listdir(export_dir)
                        # è¿™é‡Œæˆ‘ä»¬ç»Ÿè®¡æ‰€æœ‰è§£åŒ…å‡ºæ¥çš„ txt æ–‡ä»¶ï¼Œä¸ä»…ä»…æ˜¯çƒ­é—¨çš„
                        # å› ä¸ºçŽ°åœ¨æœ‰åˆ†é¡µäº†ï¼Œæ•°æ®å¤šä¸€ç‚¹ä¹Ÿæ²¡å…³ç³»
                        for tag_file in files:
                            if not tag_file.endswith(".txt"): continue
                            
                            tag_name = os.path.splitext(tag_file)[0]
                            full_path = os.path.join(export_dir, tag_file)
                            count = count_lines(full_path)
                            
                            # è®°å½•æ ¼å¼ï¼š "geoip.dat::CN": 5000
                            current_stats[author][f"{file}::{tag_name}"] = count
                                
                except Exception as e:
                    print(f"Failed to unpack {file}: {e}")

    return current_stats

def generate_reports(current_stats, old_stats):
    """ç”Ÿæˆä¸» README å’Œ ä½œè€…å­ README"""
    
    update_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # --- 1. ç”Ÿæˆä¸»é¡µ README.md (Root) ---
    root_lines = [
        "# ðŸŒ GeoData Assets Collection", 
        "", 
        f"> Last Updated: {update_time} (UTC+8)",
        "",
        "## ðŸ“‚ è§„åˆ™é›†æ¦‚è§ˆ / Overview",
        "",
        "| æ•°æ®æ¥æº (Author) | åŒ…å«è§„åˆ™é›†æ•°é‡ | è¯¦ç»†æŠ¥å‘Š |",
        "|---|---|---|"
    ]

    # --- 2. éåŽ†æ¯ä¸ªä½œè€…ï¼Œç”Ÿæˆå­é¡µ READMEï¼Œå¹¶æ›´æ–°ä¸»é¡µè¡Œ ---
    for author, rules in sorted(current_stats.items()):
        if not rules: 
            continue
            
        rule_count = len(rules)
        # ä¸»é¡µè¡¨æ ¼æ·»åŠ ä¸€è¡Œ
        # æ³¨æ„é“¾æŽ¥å†™æ³•ï¼š ./AuthorName/README.md
        root_lines.append(f"| **{author}** | {rule_count} ä¸ª | [æŸ¥çœ‹è¯¦æƒ… / View Details](./{author}/README.md) |")
        
        # --- ç”Ÿæˆå­é¡µå†…å®¹ ---
        author_lines = [
            f"# ðŸ“Š {author} - è¯¦ç»†è§„åˆ™ç»Ÿè®¡",
            "",
            f"> æ›´æ–°æ—¶é—´: {update_time}",
            f"> [ðŸ”™ è¿”å›žä¸»é¡µ / Back to Home](../README.md)", 
            "",
            "## ðŸ“ˆ è§„åˆ™å˜åŠ¨è¯¦æƒ…",
            "",
            "| è§„åˆ™æ–‡ä»¶::æ ‡ç­¾ | å½“å‰æ¡ç›®æ•° | è¾ƒæ˜¨æ—¥å˜åŒ– |",
            "|---|---|---|"
        ]
        
        # å¡«å……å­é¡µè¡¨æ ¼
        for key, count in sorted(rules.items()):
            # key æ ¼å¼ä¸º "geoip.dat::cn"
            old_count = old_stats.get(author, {}).get(key, 0)
            diff = count - old_count
            
            diff_str = "-"
            if diff > 0: 
                diff_str = f"ðŸ”º +{diff}"
            elif diff < 0: 
                diff_str = f"ðŸ”» {diff}"
            elif old_count == 0:
                diff_str = "ðŸ†• New"
            
            author_lines.append(f"| {key} | {count} | {diff_str} |")
        
        author_lines.append("")
        author_lines.append("## ðŸ“¥ å¦‚ä½•ä½¿ç”¨")
        author_lines.append(f"æ­¤ç›®å½•åŒ…å«äº† `{author}` çš„åŽŸå§‹ `.dat` æ–‡ä»¶ä»¥åŠè§£åŒ…åŽçš„æ–‡æœ¬è§„åˆ™ã€‚")
        
        # å†™å…¥å­é¡µ README
        author_readme_path = os.path.join(WORKSPACE_DIR, author, "README.md")
        with open(author_readme_path, "w", encoding='utf-8') as f:
            f.write("\n".join(author_lines))

    # å†™å…¥ä¸»é¡µ README
    root_readme_path = os.path.join(WORKSPACE_DIR, "README.md")
    with open(root_readme_path, "w", encoding='utf-8') as f:
        f.write("\n".join(root_lines))
    
    # ä¿å­˜ stats.json
    with open(STATS_FILE, "w", encoding='utf-8') as f:
        json.dump(current_stats, f, indent=2)

def main():
    print("â³ Loading old stats...")
    old_stats = {}
    if os.path.exists(OLD_STATS_FILE):
        try:
            with open(OLD_STATS_FILE, 'r') as f:
                old_stats = json.load(f)
        except:
            print("Old stats file corrupted, skipping diff.")

    print("â³ Processing assets...")
    current_stats = process_dat_files()
    
    print("â³ Generating reports...")
    generate_reports(current_stats, old_stats)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
